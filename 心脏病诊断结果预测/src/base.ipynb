{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# author=yphacker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   43    0   2       122   213    0        1      165      0      0.2      1   \n",
      "1   66    0   2       146   278    0        0      152      0      0.0      1   \n",
      "2   58    1   2       140   211    1        0      165      0      0.0      2   \n",
      "3   63    0   0       124   197    0        1      136      1      0.0      1   \n",
      "4   57    1   1       154   232    0        0      164      0      0.0      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     2     1.0  \n",
      "1   1     2     1.0  \n",
      "2   0     2     1.0  \n",
      "3   0     2     0.0  \n",
      "4   1     2     0.0  \n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "298   44    0   2       118   242    0        1      149      0      0.3   \n",
      "299   57    0   1       130   236    0        0      174      0      0.0   \n",
      "300   59    1   2       150   212    1        1      157      0      1.6   \n",
      "301   50    0   1       120   244    0        1      162      0      1.1   \n",
      "302   58    0   0       100   248    0        0      122      0      1.0   \n",
      "\n",
      "     slope  ca  thal  target  \n",
      "298      1   1     2     NaN  \n",
      "299      1   1     2     NaN  \n",
      "300      2   0     2     NaN  \n",
      "301      2   0     2     NaN  \n",
      "302      1   0     2     NaN  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "data.drop('id', axis=1, inplace=True)\n",
    "print(data.head())\n",
    "print(data.taill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n",
      "(303, 16)\n",
      "(303, 16)\n",
      "(303, 16)\n",
      "(303, 17)\n",
      "(303, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yphacker/opt/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/yphacker/opt/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "data.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'serum_cholesterol', 'fasting_blood_sugar',\n",
    "                'rest_ecg', 'max_heart_rate',\n",
    "                'exercise_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n",
    "\n",
    "# x = data.corr()\n",
    "# pd.DataFrame(x['target']).sort_values(by='target', ascending=False).style.background_gradient(cmap='copper')\n",
    "data.sex = data.sex.map({0: 'female', 1: 'male'})\n",
    "\n",
    "data.chest_pain_type = data.chest_pain_type.map(\n",
    "    {1: 'angina pectoris', 2: 'atypical angina', 3: 'non-anginal pain', 4: 'SMI', 0: 'absent'})\n",
    "\n",
    "data.fasting_blood_sugar = data.fasting_blood_sugar.map({0: 'lower than 120mg/ml', 1: 'greater than 120mg/ml'})\n",
    "\n",
    "data.exercise_angina = data.exercise_angina.map({0: 'no', 1: 'yes'})\n",
    "\n",
    "data.st_slope = data.st_slope.map({1: 'upsloping', 2: 'horizontal', 3: 'downsloping', 0: 'absent'})\n",
    "\n",
    "data.thalassemia = data.thalassemia.map({1: 'normal', 2: 'fixed defect', 3: 'reversable defect', 0: 'absent'})\n",
    "\n",
    "data.head()\n",
    "\n",
    "# X = data.iloc[:, 0:13]\n",
    "# Y = data.iloc[:, -1]\n",
    "\n",
    "num_columns = ['age', 'resting_blood_pressure', 'serum_cholesterol', 'max_heart_rate', 'st_depression']\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "# X[num_columns] = ss.fit_transform(X[num_columns])\n",
    "\n",
    "# categorical_columns = X.select_dtypes(include=['object']).head().columns\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     dummies = pd.get_dummies(X[column], drop_first=True)\n",
    "#     X[dummies.columns] = dummies\n",
    "#     X.drop(column, axis=1, inplace=True)\n",
    "#     print(X.shape)\n",
    "\n",
    "data[num_columns] = ss.fit_transform(data[num_columns])\n",
    "\n",
    "categorical_columns = data.select_dtypes(include=['object']).head().columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    dummies = pd.get_dummies(data[column], drop_first=True)\n",
    "    data[dummies.columns] = dummies\n",
    "    data.drop(column, axis=1, inplace=True)\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns.tolist()\n",
    "columns.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-Train: (242, 18)\n",
      "X-Test: (242, 18)\n",
      "Y-Train: (242,)\n",
      "Y-Test: (242,)\n"
     ]
    }
   ],
   "source": [
    "train_df = data[data['target'].notnull()]\n",
    "X = train_df[columns]\n",
    "Y = train_df['target']\n",
    "# temp = X.copy()\n",
    "# temp['target'] = Y\n",
    "\n",
    "# d = temp.corr()\n",
    "# pd.DataFrame(d['target']).sort_values(by='target', ascending=False).style.background_gradient(cmap='copper')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = X, X, Y, Y\n",
    "\n",
    "print(\"X-Train:\", X_train.shape)\n",
    "print(\"X-Test:\", X_test.shape)\n",
    "print(\"Y-Train:\", y_train.shape)\n",
    "print(\"Y-Test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on test set using Logistic Regression is: 86.8%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.82      0.85       111\n",
      "         1.0       0.86      0.91      0.88       131\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       242\n",
      "   macro avg       0.87      0.86      0.87       242\n",
      "weighted avg       0.87      0.87      0.87       242\n",
      "\n",
      "The optimal K value is with default weight parameter:  17\n",
      "Accuracy scores for each K value is :  [0.715 0.797 0.789 0.81  0.802 0.814 0.81  0.806 0.823 0.814 0.802 0.81\n",
      " 0.814 0.814 0.81 ]\n",
      "The accuracy on test set using KNN for optimal K = 17 is 83.884%\n",
      "The accuracy on test set using SVC is: 90.10000000000001%\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.2min finished\n",
      "/Users/yphacker/opt/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/model_selection/_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on test set using RandomForest is: 96.3%\n",
      "Training the LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Training the KNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=-1, n_neighbors=17, p=2,\n",
      "           weights='uniform')\n",
      "Training the RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1200, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Training the SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "The accuracy on test set using voting classifier is 89.7%\n",
      "The accuracy on test set using voting classifier is 90.91%\n"
     ]
    }
   ],
   "source": [
    "# def show_metrics(model):\n",
    "#     fig = plt.figure(figsize=(25, 10))\n",
    "\n",
    "#     # Confusion matrix\n",
    "#     fig.add_subplot(121)\n",
    "#     sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "\n",
    "#     # ROC Curve\n",
    "#     fig.add_subplot(122)\n",
    "\n",
    "#     auc_roc = roc_auc_score(y_test, model.predict(X_test))\n",
    "#     fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=2, marker='o', label='Trained Model (area = {0:0.3f})'.format(auc_roc))\n",
    "#     plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--', label='No Skill (area = 0.500)')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver operating characteristic')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# creating our model instance\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# fitting the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# predicting the target vectors\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# show_metrics(log_reg)\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"The accuracy on test set using Logistic Regression is: {np.round(accuracy, 3) * 100.0}%\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# creating a list of K's for performing KNN\n",
    "my_list = list(range(0, 30))\n",
    "\n",
    "# filtering out only the odd K values\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, my_list))\n",
    "\n",
    "# list to hold the cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation with default weights\n",
    "for k in neighbors:\n",
    "    Knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n",
    "    scores = cross_val_score(Knn, X_train, y_train, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# finding the optimal k\n",
    "optimal_k = neighbors[cv_scores.index(max(cv_scores))]\n",
    "print(\"The optimal K value is with default weight parameter: \", optimal_k)\n",
    "\n",
    "# # plotting accuracy vs K\n",
    "# plt.plot(neighbors, cv_scores)\n",
    "# plt.xlabel(\"Number of Neighbors K\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Accuracy vs K Plot for normal \")\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "print(\"Accuracy scores for each K value is : \", np.round(cv_scores, 3))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create instance of classifier\n",
    "knn_optimal = KNeighborsClassifier(n_neighbors=optimal_k, algorithm='kd_tree',\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "knn_optimal.fit(X_train, y_train)\n",
    "\n",
    "# predict on test vector\n",
    "y_pred = knn_optimal.predict(X_test)\n",
    "\n",
    "# evaluate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "print(f\"The accuracy on test set using KNN for optimal K = {optimal_k} is {np.round(accuracy, 3)}%\")\n",
    "\n",
    "# show_metrics(knn_optimal)\n",
    "\n",
    "# Creating an instance of the classifier\n",
    "svm = SVC(gamma='scale')\n",
    "\n",
    "# training on train data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# predicting on test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# let's look at our accuracy\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"The accuracy on test set using SVC is: {np.round(accuracy, 3) * 100.0}%\")\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2,\n",
    "                               random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "rf_random.best_params_\n",
    "\n",
    "# Creating an instance for the classifier\n",
    "rf_best = RandomForestClassifier(**rf_random.best_params_)\n",
    "\n",
    "# fitting the model\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels\n",
    "y_pred = rf_best.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"The accuracy on test set using RandomForest is: {np.round(accuracy, 3) * 100.0}%\")\n",
    "\n",
    "# creating a list of our models\n",
    "ensembles = [log_reg, knn_optimal, rf_best, svm]\n",
    "\n",
    "# Train each of the model\n",
    "for estimator in ensembles:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "scores = [estimator.score(X_test, y_test) for estimator in ensembles]\n",
    "\n",
    "scores\n",
    "\n",
    "named_estimators = [\n",
    "    (\"log_reg\", log_reg),\n",
    "    ('random_forest', rf_best),\n",
    "    ('svm', svm),\n",
    "    ('knn', knn_optimal),\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Let's look at our accuracy\n",
    "acc = voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"The accuracy on test set using voting classifier is {np.round(acc, 3) * 100}%\")\n",
    "\n",
    "# to generate permutations of length three\n",
    "perm = permutations(named_estimators, 3)\n",
    "\n",
    "# to store the acc and classifiers\n",
    "best_perm = []\n",
    "\n",
    "# to store best classifier\n",
    "best = []\n",
    "\n",
    "# Traverse through the obtained permutations\n",
    "for i in list(perm):\n",
    "    # fit the classifier\n",
    "    voting_clf = VotingClassifier(i)\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "\n",
    "    # obtain accracy score and append it to the list\n",
    "    acc = voting_clf.score(X_test, y_test)\n",
    "    best_perm.append([acc, voting_clf])\n",
    "\n",
    "# find out the maximum accuracy\n",
    "maximum = max(best_perm, key=lambda x: x[0])\n",
    "\n",
    "# there can be multiple permutations for which we get\n",
    "# best score so find all of them and append to best\n",
    "for i in range(len(best_perm)):\n",
    "    if maximum[0] == best_perm[i][0]:\n",
    "        best.append(best_perm[i][1])\n",
    "\n",
    "acc_scores = []\n",
    "\n",
    "for i in range(len(best)):\n",
    "    voting_clf = best[i]\n",
    "    # fit the classifier\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    # Let's look at our accuracy\n",
    "    acc_scores.append(voting_clf.score(X_test, y_test))\n",
    "\n",
    "print(f\"The accuracy on test set using voting classifier is {np.round(max(acc_scores), 4) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yphacker/opt/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "X = data[data['target'].isnull()]\n",
    "preds = voting_clf.predict(X[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf_best.predict(X[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0     0.0\n",
       "1   1     1.0\n",
       "2   2     1.0\n",
       "3   3     0.0\n",
       "4   4     0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'id': test_df['id'], 'target': preds})\n",
    "submission.to_csv(\"../data/submission.csv\", index=False, header=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
